{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Group Project\n",
    "\n",
    "Issued: June 11, 2024\n",
    "\n",
    "Due: July 22, 2024\n",
    "\n",
    "Please submit a link to your code base (ideally with a branch that does not change anymore after the submission deadline) and your 4-page report via email to carsten.eickhoff@uni-tuebingen.de by the due date. One submission per team.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Crawling & Indexing\n",
    "Crawl the web to discover **English content related to Tübingen**. The crawled content should be stored locally. If interrupted, your crawler should be able to re-start and pick up the crawling process at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports ####\n",
    "import requests\n",
    "import copy\n",
    "from boilerpy3 import extractors\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, download\n",
    "import re\n",
    "from trafilatura import fetch_url, extract\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from duplicateCheck import check_simhash,computeHash\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser as rp\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import html5lib\n",
    "import requests\n",
    "import stopit\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from langdetect import detect\n",
    "import random\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import logging\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "# Load the German and english dictionary\n",
    "spellGerman = SpellChecker(language='de')\n",
    "spellEnglish = SpellChecker(language='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##returns corresponding wordnet POS tag for a nltk pos tag\n",
    "# returns none if no lemmatization necessary\n",
    "def getWordnetPos(nltkPos):\n",
    "   if nltkPos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "   elif nltkPos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "   elif nltkPos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "   elif nltkPos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "   else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "#### Preprocessing of terms ####\n",
    "##preprocesses a term t \n",
    "#returns preprocessed t or False in case term is uninformative\n",
    "def preprocess(t, tag):\n",
    " #get nltk stopwords english and german! (often also german stopwords still contained)\n",
    " stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n",
    "\n",
    " #initialize lemmatizer\n",
    " lemmatizer = WordNetLemmatizer()\n",
    "\n",
    " #convert to lowercase\n",
    " t = t.lower()\n",
    "\n",
    " #is url (starting with http(s) or www.) or empty\n",
    " if re.compile(r'https?://\\S+|www\\.\\S+|(\\s)*\\.(org|de|com)|^$').match(t):\n",
    "    return False\n",
    "      \n",
    " # special character,punctuation -> continue in this case \n",
    " if re.compile(r'[^a-zA-Z0-9äöüÄÖÜ\\s]').match(t):\n",
    "    return False\n",
    " \n",
    " #is stopword\n",
    " if t in stop_words:\n",
    "    return False \n",
    " \n",
    "\n",
    " #filter out words that are for sure not english\n",
    " if (t in spellGerman):\n",
    "     if(not t in spellEnglish): \n",
    "       return False\n",
    "     \n",
    " \n",
    " #remove also special chars inside token\n",
    " t = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s]', '', t)\n",
    "      \n",
    " #convert pos tag for lemmatization\n",
    " ltag = getWordnetPos(tag)\n",
    " if ltag:\n",
    "   t = lemmatizer.lemmatize(t, ltag)\n",
    "\n",
    " return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the directory for documents and pictures \n",
    "def create_directories():\n",
    "    base_dir = 'static'\n",
    "    directories = ['documents', 'pictures']\n",
    "\n",
    "    for directory in directories:\n",
    "        dir_path = os.path.join(base_dir, directory)\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            print(f\"directory '{dir_path}' created.\")\n",
    "        else:\n",
    "            print(f\"directory '{dir_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Indexing ####\n",
    "\n",
    "#Add a document to the index. You need (at least) two parameters:\n",
    "    #doc: The document to be indexed.\n",
    "    #index: The location of the local index storing the discovered documents.\n",
    "def index(doc, index):\n",
    " url = doc['url']\n",
    " indexPath = index\n",
    "\n",
    " #get index of index.json\n",
    " with open(indexPath, 'r', encoding='utf-8') as f:\n",
    "     index = json.load(f)\n",
    "     docID = len(index[1])\n",
    "     print(docID)\n",
    "     doc = doc['doc']\n",
    "     f.close()\n",
    "\n",
    "\n",
    "####  Text Preprocessing of the doc ###################\n",
    "\n",
    " #extract relevant text from raw html doc\n",
    " content = extract(doc)\n",
    "\n",
    " # proof if english -> break if not\n",
    " # test on first 200 Words (if available)\n",
    " splittedText = content.split()\n",
    " if len(splittedText) >= 200:\n",
    "     testText = ' '.join(splittedText[:200])\n",
    " else:\n",
    "     testText = ' '.join(splittedText[:len(splittedText)])\n",
    " if(detect(testText) != \"en\"):\n",
    "      return [False, docID]\n",
    "\n",
    "\n",
    "###storing unprocessed text content ##### -> store as file at end if no duplicate\n",
    " docContents={}\n",
    " soup = bs(doc, 'html.parser')\n",
    " \n",
    " title = re.sub(r'[\\n\\t]', '', soup.find('title').text) if soup.find('title') else 'No title'\n",
    " docContents['title'] = title\n",
    " meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "\n",
    " ###if description available save description, else save first 40 words (or less if doc shorter) -> remove special characters from that ####\n",
    " #take document description if available and at least 10 words long and in english else take first 40 words of first sensful paragraph as preview\n",
    " if meta_desc and (len(meta_desc['content'].split()) >=10) and detect(meta_desc['content']) ==\"en\":\n",
    "     docContents['text'] = meta_desc['content'] \n",
    " else:\n",
    "    #find a paragraph and take first 40 of it. Else take arbitrary first 40\n",
    "    for p in soup.findAll('p'):\n",
    "           beginning = p.text.split()\n",
    "           if len(beginning) >=40:\n",
    "               splittedText = beginning\n",
    "               break\n",
    "    #splittedText = soup.find('p').text.split() if soup.find('p') else splittedText\n",
    "    if len(splittedText) >= 40:\n",
    "     docContents['text'] = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s!\"#$%&\\'()*+,\\-\\./:;<=>?@[\\\\]^_`{|}~]', '', ' '.join(splittedText[:40]))\n",
    "    else: \n",
    "     docContents['text'] = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s!\"#$%&\\'()*+,\\-\\./:;<=>?@[\\\\]^_`{|}~]', '', ' '.join(splittedText[:len(splittedText)]))\n",
    "\n",
    "    \n",
    "    #append \"...\" if text not ending at \".\"\n",
    "    if docContents['text'][-1] != \".\":\n",
    "        docContents['text'] += \"...\"\n",
    "        \n",
    " docContents['text'] =  re.sub(r'[\\n\\t\\r]', ' ', docContents['text'])\n",
    "\n",
    " #docContents['content'] = content\n",
    " # Find the first image\n",
    " first_image = soup.find('img')\n",
    " \n",
    " \n",
    " #convert to lowercase\n",
    " content = content.lower()\n",
    "\n",
    " #remove contractions:\n",
    " content = contractions.fix(content)\n",
    " \n",
    "\n",
    " #store words of the processed document\n",
    " processedDoc = []\n",
    " #tokenize, go through all words and do the steps for each at once\n",
    " for position, (t, tag) in enumerate(pos_tag(word_tokenize(content))):\n",
    "    \n",
    "      #break up hyphenated words:\n",
    "       if '-' in t:\n",
    "          t = t.split('-')\n",
    "          firstTermPreprocessed = preprocess(t[0], tag)\n",
    "          #if not uninformative\n",
    "          if firstTermPreprocessed:\n",
    "              processedDoc.append([firstTermPreprocessed, position])\n",
    "          secondTermPreprocessed = preprocess(t[1], tag)\n",
    "          if secondTermPreprocessed:\n",
    "              processedDoc.append([secondTermPreprocessed, position])\n",
    "       else:\n",
    "           termPreprocessed = preprocess(t, tag)\n",
    "           if termPreprocessed:\n",
    "               processedDoc.append([termPreprocessed, position])\n",
    "\n",
    "    \n",
    "      \n",
    "  ############# Near Duplicate checking ##############\n",
    "  #near duplicate checking by means of the words in the processed document (processedDoc)\n",
    "\n",
    " docHash = computeHash([l[0] for l in processedDoc])\n",
    " # compare to other doc hashes\n",
    " if(check_simhash(docHash, index[2])):\n",
    "     #break and dont index if is near duplicate\n",
    "     return [False, 0]\n",
    " #if no duplicate save hash and insert terms in inverted index\n",
    " index[2].append(docHash)\n",
    "\n",
    "\n",
    "\n",
    "############ Build up inverted index #####################\n",
    " for term, position in processedDoc:\n",
    "     \n",
    "      #entry in inverted index is: [docID, [occurence1, occurence2, ...]]\n",
    "      #add on thy fly to inverted index\n",
    "      #if word already in index add doc idto the words list\n",
    "      if(term in index[0].keys()):\n",
    "           #check if doc id already in list of that word (must be at the end)\n",
    "           if index[0][term][len(index[0][term])-1][0] == docID:\n",
    "                #if so append position there\n",
    "                index[0][term][len(index[0][term])-1][1].append(position)\n",
    "           #else add new list [docID, [position of t],skip pointer, tfidfval] for that word\n",
    "           else:\n",
    "               index[0][term].append([docID, [position], None])\n",
    "               \n",
    "      #if word not yet in index add \"t: [[docID, [get position of t], tfidf weight for t in d, skip pointer (None) , tfidfval]]\" to dict\n",
    "      else:\n",
    "           index[0][term] = [[docID, [position], None]]\n",
    "\n",
    " length = len(processedDoc)\n",
    "\n",
    " #add url , cluster of document (None so far) and length of preprocessed doc to list index[1] after indexing the doc\n",
    " index[1].append([url, None, length])\n",
    "\n",
    " #repeat saving until it suceeded (for case of timeout exception)\n",
    " while True:\n",
    "    try:     \n",
    "        #write changed index object to json\n",
    "        with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index, f, ensure_ascii=False)\n",
    "        \n",
    "        #save doc in documents ordner: name <docID>.json\n",
    "        with open(os.path.join(os.getcwd(), \"static\", \"documents\", str(docID) + \".json\"), 'w', encoding='utf-8') as f:\n",
    "            json.dump(docContents, f, ensure_ascii=False)\n",
    "        \n",
    "        #save first image of page if found\n",
    "        if first_image:\n",
    "            img_url = first_image['src']\n",
    "            # Convert relative URL to absolute URL\n",
    "            absolute_image_url = urljoin(url, img_url)\n",
    "            f_ext = os.path.splitext(absolute_image_url)[-1]\n",
    "            image_fetched = requests.get(absolute_image_url)\n",
    "            if image_fetched.status_code == 200:\n",
    "            # Get the content of the image\n",
    "             image_content = image_fetched.content\n",
    "             #save img in pictures ordner: name <docID>\n",
    "             with open(os.path.join(os.getcwd(), \"static\" ,\"pictures\", str(docID)) + f_ext, 'wb') as f:\n",
    "              f.write(image_content)\n",
    "        print('indexed')\n",
    "        return [True, docID]\n",
    "    except stopit.TimeoutException as e:\n",
    "         print(\"Error at saving index: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Clustering ####\n",
    "#clusters the docs of the index and inserts the labels into the index (currently 15 clusters)\n",
    "# cluster using kmeans clustering with tf-idf vectors\n",
    "def cluster(index):\n",
    "     print('start clustering')\n",
    "     indexPath = index\n",
    "     #get index of index.json\n",
    "     f = open(indexPath, encoding='utf-8')\n",
    "     index = json.load(f)\n",
    "     #convert index to tf-idf vector representations for each document\n",
    "     idx = index[0] \n",
    "     docs = index[1]\n",
    "     #matrix to store vectors (rows: documents , cols: terms)\n",
    "     tfIDFMatrix = np.zeros((len(docs), len(idx.keys())))\n",
    "     print('buildMatrix')\n",
    "\n",
    "     for t in range(len(idx.keys())):\n",
    "         term = list(idx.keys())[t]\n",
    "         for i in range(len(idx[list(idx.keys())[t]])):\n",
    "             #term index: t, doc index: idx[t][i][0]\n",
    "\n",
    "             docID = idx[term][i][0]\n",
    "             \n",
    "             #calculate tf (nr occurences of t in doc/ length of doc) * idf(log (#docs/ #docs containing t))\n",
    "             # occurences of t in doc: len(idx[term][i][1])\n",
    "             # length of doc: docs[docID][2]\n",
    "             # # docs = len[docs]\n",
    "             # #docs containing term = len(idx[term])\n",
    "             tfValue = len(idx[term][i][1])/docs[docID][2]\n",
    "             #idfValue = math.log(len(docs)/len(idx[term]))\n",
    "             tfIDFMatrix[docID][t] = tfValue #* idfValue\n",
    "\n",
    "             ## store idf value for that doc and term in index on the fly (for BM25 later on)\n",
    "             #index[0][term][i][3] = idfValue\n",
    "     \n",
    "     print('endBuildmatrix')\n",
    "\n",
    "     #also calculate avg doc length and append to index\n",
    "     docLengths = [l[2] for l in index[1]]\n",
    "     avgLength = sum(docLengths)/len(docLengths)\n",
    "     index[3] = avgLength\n",
    "      \n",
    "     #place skip pointers once after whole index is built up\n",
    "     #### rearange skip pointers ###########\n",
    "     #delete current skip pointers for that posting list\n",
    "     for term in index[0].keys():\n",
    "        # sqare |p| evenly spaced per posting list (p: length of posting list of term t)\n",
    "        p = len(index[0][term])\n",
    "        #just if posting list has at least length 4\n",
    "        if(p >= 4):\n",
    "            nrPointers = math.sqrt(p)\n",
    "            spaceBetween = math.floor(p/nrPointers)\n",
    "            #current index in postingslist\n",
    "            i = 0\n",
    "            while(i + spaceBetween < p):\n",
    "                #set skip pointer [idx to jump to in postings list, docID at that index]\n",
    "                index[0][term][i][2] = [i + spaceBetween, index[0][term][i + spaceBetween][0]]\n",
    "                i += spaceBetween\n",
    "\n",
    "     #store new  index with tf idf values \n",
    "     #write changed index object to json\n",
    "     with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "      json.dump(index, f, ensure_ascii=False)\n",
    "     # Initialize KMeans clustering\n",
    "     print('clustering')\n",
    "     num_clusters = 30 # Example: Number of clusters\n",
    "     kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "     # Fit KMeans model to the TF-IDF matrix\n",
    "     kmeans.fit(tfIDFMatrix)\n",
    "\n",
    "     docLabels = kmeans.labels_\n",
    "     #centroids of each cluster:\n",
    "     centroids = kmeans.cluster_centers_\n",
    "\n",
    "     ###get term for each cluster #####\n",
    "      \n",
    "     #get term with highest tf-idf score per cluster centroid\n",
    "     centroidTopics = []\n",
    "     for centroid in centroids:\n",
    "\n",
    "        ###get x terms with hightest tf-idf value\n",
    "        # indices for words in descending order (regarding tf-idf val)\n",
    "        sorted_indices = np.argsort(centroid)[::-1]\n",
    "        #take top 50 words:\n",
    "        top50Indices = sorted_indices[:50]\n",
    "        #words of these indices\n",
    "        top50Words = [list(idx.keys())[l] for l in top50Indices]\n",
    "    \n",
    "\n",
    "       ### take word with highest tf value in centroid as topic for each cluster ###\n",
    "       # search for first sensful word (more than one char and not only number) and not tübingen\n",
    "        topic = ' '\n",
    "        for i in range(len(top50Words)):\n",
    "         if len(top50Words[i])>1 and not top50Words[i].isnumeric() and top50Words[i] != \"tübingen\":\n",
    "          topic = top50Words[i].capitalize()\n",
    "          break\n",
    "        #print(\"centroid words: \" + str(top50Words))\n",
    "        #print(topic)\n",
    "        centroidTopics.append(topic)\n",
    "     \n",
    "     \n",
    "     #insert labels in index\n",
    "     for d in range(len(index[1])):\n",
    "         index[1][d][1] = centroidTopics[docLabels[d]]\n",
    "      \n",
    "     print('endClustering')\n",
    "        \n",
    "      #write changed index object to json\n",
    "     with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "      json.dump(index, f, ensure_ascii=False)\n",
    "     \n",
    "     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Crawling ####\n",
    "#Crawl the web. You need (at least) two parameters:\n",
    "#frontier: The frontier of known URLs to crawl. You will initially populate this with your seed set of URLs and later maintain all discovered (but not yet crawled) URLs here.\n",
    "#index: The location of the local index storing the discovered documents. \n",
    "STORAGE_LOC = \"index.json\"\n",
    "FRONTIER_LOC = \"frontier.json\"\n",
    "MAX_DOCS = 800\n",
    "reshuffle_at = 11\n",
    "\n",
    "\n",
    "def crawl(frontier, indexPath):\n",
    "    \n",
    "    f = open(frontier, encoding='utf-8')\n",
    "    frontierObj = json.load(f)\n",
    "    frontier = frontierObj[\"frontier\"]\n",
    "    links_seen = frontierObj[\"linksSeen\"]\n",
    "    f.close()\n",
    "   \n",
    "    \n",
    "    #get  first document of frontier while frontier not empty\n",
    "    while len(frontier) != 0:\n",
    "        with stopit.ThreadingTimeout(20) as context_manager:\n",
    "            link = frontier.pop(0)\n",
    "            print(link)\n",
    "            try:\n",
    "                base_link = urlparse(link).netloc\n",
    "                \n",
    "                # check if we are allowed to access the website\n",
    "                robots_file_loc = \"http://\" + base_link + \"/robots.txt\"\n",
    "                session = requests.Session()\n",
    "                retry = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('https://', adapter)\n",
    "                robots_file = session.get(robots_file_loc, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
    "                session.close()\n",
    "                if robots_file.ok:\n",
    "                    robot_parser = rp()\n",
    "                    robot_parser.set_url(robots_file_loc)\n",
    "                    robot_parser.read()\n",
    "                    if not robot_parser.can_fetch(\"*\", link):\n",
    "                        continue\n",
    "                elif robots_file.status_code != 404:\n",
    "                    continue\n",
    "                \n",
    "                session = requests.Session()\n",
    "                retry = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('https://', adapter)\n",
    "                document = session.get(link, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}).text\n",
    "                session.close()\n",
    "                html5_doc = html5lib.parse(document)\n",
    "                soup = bs(document, \"html.parser\")\n",
    "\n",
    "                # save and use document if it's not a duplicate\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract() \n",
    "\n",
    "                # check if document is relevant\n",
    "                doc_lang = html5_doc.get(\"lang\")\n",
    "                if doc_lang == None:\n",
    "                    doc_lang = html5_doc.get(\"xml:lang\")\n",
    "                if doc_lang == None:\n",
    "                    continue\n",
    "                relevant = soup.find(\"body\").text.find(\"Tübingen\") > -1 # type: ignore\n",
    "                if (doc_lang is not None and doc_lang.count(\"en\") > 0 and relevant): # type: ignore\n",
    "                    # process doc to right format and index\n",
    "                    doc = {\"url\": link, \"doc\": document }\n",
    "                    # TODO: index document\n",
    "                    duplicate, docAmount = index(doc, indexPath)\n",
    "                    if duplicate == False:\n",
    "                        print(\"Duplicate/ Not English\")\n",
    "                        continue\n",
    "                    # get all links from document and save to frontier if not seen yet\n",
    "                    for a in soup.find_all('a'):\n",
    "                        if (a.get('href')):\n",
    "                            l = a.get('href')\n",
    "                            if l.startswith('#'):\t\n",
    "                                continue\n",
    "                            if urlparse(l).netloc == '':\n",
    "                                l = base_link + l\n",
    "                            if urlparse(l).scheme == '':\n",
    "                                l = urlparse(link).scheme + '://' + l\n",
    "                            if l not in links_seen:\n",
    "                                frontier.append(l)\n",
    "                                links_seen.append(l)\n",
    "                    \n",
    "                    if docAmount >= MAX_DOCS:\n",
    "                        break\n",
    "                    global reshuffle_at\n",
    "                    if docAmount == reshuffle_at:\n",
    "                        random.shuffle(frontier)\n",
    "                        reshuffle_at = len(links_seen)\n",
    "                    \n",
    "                #save frontier and links seen\n",
    "                with open(FRONTIER_LOC, 'w', encoding='utf-8') as f:\n",
    "                    json.dump({\"frontier\": frontier, \"linksSeen\": links_seen}, f, ensure_ascii=False)\n",
    "                   \n",
    "\n",
    "                    \n",
    "            except Exception as err:\n",
    "                if context_manager.state == context_manager.TIMED_OUT:\n",
    "                    print(\"Timed out at link: \", link)\n",
    "                else:\n",
    "                    print(\"Exception occured at link: \", link, \". Description: \", err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset before crawling\n",
    "Execute the following cell in case you want to restart the whole crawling process and don't just continue where you stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Execution (crawling, indexing, clustering) ####\n",
    "## delete content of documents and pictures folder if there is some\n",
    "# delete index\n",
    "# also set frontier/ links seen back to initial frontier/links seen\n",
    "# so the process can be started again without resetting everything by hand\n",
    "def reset():\n",
    "    docs = [os.path.join(os.getcwd(), \"static\", \"documents\", f) for f in os.listdir(os.path.join(os.getcwd(), \"static\",  \"documents\"))]\n",
    "    for f in docs:\n",
    "        os.remove(f)\n",
    "\n",
    "    images = [os.path.join(os.getcwd(), \"static\", \"pictures\", f) for f in os.listdir(os.path.join(os.getcwd(), \"static\", \"pictures\"))]\n",
    "    for i in images:\n",
    "        os.remove(i)\n",
    "\n",
    "    #write [{}, [], [], None] in index.json \n",
    "    json_data = json.dumps([{}, [], [], None])\n",
    "    with open(STORAGE_LOC, 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "        json_file.close()\n",
    "    \n",
    "    #write initial frontier to frontier.json\n",
    "    initalLinks = [    \n",
    "            \"https://www.tuebingen.de/en/\",  \n",
    "            \"https://is.mpg.de/\",  \n",
    "            \"https://www.iwm-tuebingen.de/www/en/index.html\",\n",
    "            \"https://kunsthalle-tuebingen.de/en/\",\n",
    "            \"https://www.dzne.de/en/about-us/sites/tuebingen/\",\n",
    "            \"https://www.tuebingen-info.de/en\",\n",
    "            \"https://uni-tuebingen.de/en/\",\n",
    "            \"https://www.eventbrite.com/d/germany--tuebingen/events/\",  \n",
    "            \"https://www.opentable.com/s/?city=Tuebingen&country=Germany\",  \n",
    "            \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=T%C3%BCbingen%2C+Germany\", \n",
    "            \"https://en.wikipedia.org/wiki/T%C3%BCbingen\", \n",
    "            \"https://www.tagblatt.de/\",\n",
    "            \"https://www.nomadicmatt.com/travel-blogs/tubingen/\",\n",
    "            \"https://www.life-in-germany.net/tubingen/\",\n",
    "            \"https://www.reddit.com/r/germany/\",\n",
    "            \"https://tuebingenresearchcampus.com/en/tuebingen\",\n",
    "            \"https://tunewsinternational.com/category/news-in-english/\",\n",
    "            \"https://www.germany.travel/en/cities-culture/tuebingen.html\",\n",
    "            \"https://www.opentable.com/food-near-me/stadt-tubingen-germany\",\n",
    "            \"https://historicgermany.travel/historic-germany/tubingen/\"\n",
    "            \"https://en.wikipedia.org/wiki/University_of_T%C3%BCbingen\"]\n",
    "    \n",
    "    with open(FRONTIER_LOC, 'w') as frontier_file:\n",
    "        frontier_file.write(json.dumps({\"frontier\":initalLinks, \"linksSeen\": initalLinks}))\n",
    "        frontier_file.close()\n",
    "\n",
    "reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### crawl ####\n",
    "create_directories()\n",
    "crawl(FRONTIER_LOC, STORAGE_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster, add additional infos to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after crawling cluster and add remaining infos to index\n",
    "cluster(STORAGE_LOC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Processing \n",
    "Process a textual query and return the 100 most relevant documents from your index. Please incorporate **at least one retrieval model innovation** that goes beyond BM25 or TF-IDF. Please allow for queries to be entered either individually in an interactive user interface (see also #3 below), or via a batch file containing multiple queries at once. The batch file will be formatted to have one query per line, listing the query number, and query text as tab-separated entries. An example of the batch file for the first two queries looks like this:\n",
    "\n",
    "```\n",
    "1   tübingen attractions\n",
    "2   food and drinks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process query\n",
    "# remove stop words, lemmatize, etc.\n",
    "def prepare_query(query):\n",
    "\t\"\"\"\n",
    "\tFunction takes the query and processes it for further calculations the same way as it is done for documents for indexing.\n",
    "\tFunction removes stop words and dublicates, tokenizes and lematizes the query  \n",
    "\tArgs:\n",
    "\t\tquery: our initial query given by the user\n",
    "\n",
    "\tReturns: \n",
    "\t\tquery_tokenized: list of the processed query items\n",
    "\n",
    "\t\"\"\"\n",
    "\tquery_tokenized= []\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tquery_lower = query.lower()\n",
    "\tfor term, tag in pos_tag(word_tokenize(query_lower)):\n",
    "\t\tif re.compile(r'[^a-zA-Z0-9äöüÄÖÜ\\s]').match(term):\n",
    "\t\t\tcontinue\n",
    "\t\tif term in stop_words:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# split words with \"-\"\n",
    "\t\tif '-' in term:\n",
    "\t\t\tt = term.split('-')\n",
    "\t\t\tfor i, tag in pos_tag(t):\n",
    "\t\t\t\tltag = getWordnetPos(tag)\n",
    "\t\t\t\tif ltag:\n",
    "\t\t\t\t\ti = lemmatizer.lemmatize(i, ltag)\t\n",
    "\t\t\t\tif i not in query_tokenized:\n",
    "\t\t\t\t\tquery_tokenized.append(i)\n",
    "\n",
    "\t\telse: # words without \"-\"\n",
    "\t\t\tltag = getWordnetPos(tag)\n",
    "\t\t\tif ltag :\n",
    "\t\t\t\tterm = lemmatizer.lemmatize(term, ltag)\t\n",
    "\t\t\tif term not in query_tokenized:\n",
    "\t\t\t\tquery_tokenized.append(term)\n",
    "\treturn query_tokenized\n",
    "\n",
    "#get all the documents that include all the queryterms from our index \n",
    "def get_documents_from_index(query_tokenized, index):\n",
    "\t\"\"\"\n",
    "\tFunction takes the query and the index, searches for documents in index which contain all the query-terms- \n",
    "\tArgs:\n",
    "\t\tquery_tokenized: our processed query \n",
    "\t\tindex: index containing all terms and matching documents\n",
    "\n",
    "\tReturns: \n",
    "\t\tdocuments: list of all documents matching our query\n",
    "\n",
    "\t\"\"\"\n",
    "\tdocuments = []\n",
    "\tfirst_document = True\n",
    "\tsecond_document = False\n",
    "\n",
    "\tfor term in query_tokenized:\n",
    "\t\t# first term in query --> get all documents amtching this term from index\n",
    "\t\tif term in index.keys() and first_document:   \t\t\n",
    "\t\t\tfirst_document = False\t\n",
    "\t\t\tdocuments = index[term]\n",
    "\t\t\tsecond_document = True\n",
    "\t\t# check second term. get documents from first iteration which have this term as well. use skip pointers for both lists \n",
    "\t\telif term in index.keys() and second_document: \n",
    "\t\t\tsecond_document = False\n",
    "\t\t\tmatches = []\n",
    "\t\t\tnew_term_list = index[term]\n",
    "\t\t\ti = 0\n",
    "\t\t\tj = 0\n",
    "\t\t\twhile i < len(documents) and j < len(new_term_list):  \n",
    "\t\t\t\tif documents[i][0] == new_term_list[j][0]:\n",
    "\t\t\t\t\tmatches.append(documents[i])\t\t\t\t\t\t\t# append entry including skip pointers for future iterations \n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tj += 1\n",
    "\t\t\t\telif documents[i][0] < new_term_list[j][0]:                 # A < B\n",
    "\t\t\t\t\tif documents[i][2]:                                     # If there is a skip pointer\n",
    "\t\t\t\t\t\tif documents[i][2][1] <= new_term_list[j][0]:       # Take it if it does not carry you beyond the id pointed to by B\n",
    "\t\t\t\t\t\t\ti = documents[i][2][0]\n",
    "\t\t\t\t\t\telse:                                       \t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\t\t\t\ti += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ti += 1\n",
    "\t\t\t\telif new_term_list[j][2]:                                   # If there is a skip pointer\n",
    "\t\t\t\t\tif new_term_list[j][2][1] <= documents[i][0]:           # Take it if it does not carry you beyond the id pointed to by A\n",
    "\t\t\t\t\t\tj = new_term_list[j][2][0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tj += 1                                     \t\t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tj += 1 \n",
    "\t\t\tdocuments = matches\t\t\n",
    "\t\t\t\n",
    "\t\t# check current term. get documents from previous iteration which have this term as well. use skip pointers only for list in index.\n",
    "\t\telif term in index.keys():\n",
    "\t\t\tmatches = []\n",
    "\t\t\tnew_term_list = index[term]\n",
    "\t\t\ti = 0\n",
    "\t\t\tj = 0\n",
    "\t\t\twhile i < len(documents) and j < len(new_term_list):  \n",
    "\t\t\t\tif documents[i][0] == new_term_list[j][0]:\n",
    "\t\t\t\t\tmatches.append(documents[i])\t\t\t\t\t\t\t# append entry including skip pointers for future iterations \n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tj += 1\n",
    "\t\t\t\telif documents[i][0] < new_term_list[j][0]:    \t\t\t\t# A < B\n",
    "\t\t\t\t\ti += 1\t\t\t\t\t\t\t\t\t\t\t\t\t# This time no skip pointers, because they aren't correct anymore\n",
    "\n",
    "\t\t\t\telif new_term_list[j][2]:                                   # If there is a skip pointer\n",
    "\t\t\t\t\tif new_term_list[j][2][1] <= documents[i][0]:           # Take it if it does not carry you beyond the id pointed to by A\n",
    "\t\t\t\t\t\tj = new_term_list[j][2][0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tj += 1                                     \t\t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tj += 1 \n",
    "\t\t\tdocuments = matches\t\t\n",
    "\t\telse:\n",
    "\t\t\treturn []\t\t\t\t\t\t\t\t\t\t\t\t\t\t# query term wasn't found, no document can satisfy query\n",
    "\n",
    "\t# get rid of skip pointers:\n",
    "\ttmp = []\n",
    "\tfor element in documents:\n",
    "\t\ttmp.append(element[0])\n",
    "\tdocuments = tmp\n",
    "\n",
    "\treturn documents\n",
    "\n",
    "# calculate bm25 score for each document\n",
    "def bm25(document, query, index, k, b, avg_length, document_length, doc_num) :\n",
    "\t\"\"\"\n",
    "\tFunction calculates the bm25 score for a document-query pair.  \n",
    "\tArgs:\n",
    "\t\tdocument: a document matching our query\n",
    "\t\tquery: our initial query given by the user\n",
    "\t\tindex: index containing all terms and matching documents\n",
    "\t\tk: parameter for optimization\n",
    "\t\tb: parameter for optimization\n",
    "\t\tavg_length: average length of all documents in our index\n",
    "\t\tdocument_length: length of the document\n",
    "\t\tdoc_num: total number of documents in our index\n",
    "\n",
    "\tReturns: \n",
    "\t\tscore: bm25 score for our document-query pair\n",
    "\n",
    "\t\"\"\"\n",
    "\tscore = 0\n",
    "\t\n",
    "\t#calculate score:\n",
    "\tfor term in query:\n",
    "\t\ttf = 0\n",
    "\t\tidf = 0\n",
    "\t\t# get tf from index\n",
    "\t\tfor doc in index[term]:\n",
    "\t\t\tif doc[0] == document:\n",
    "\t\t\t\ttf = len(doc[1])\n",
    "\n",
    "\t\t#calculate idf:\n",
    "\t\tidf = math.log(doc_num/len(index[term]))\n",
    "\n",
    "\t\tnumerator = tf *(k+1)\n",
    "\t\tdenominator = tf + (k*(1 - b + (b* (document_length/avg_length))))\n",
    "\t\t#add score of query-term to score:\n",
    "\t\tscore += idf * (numerator/denominator)\n",
    "\n",
    "\treturn score\n",
    "\n",
    "\n",
    "def get_distance(document, query, index):\n",
    "\t\"\"\"\n",
    "\tFunction minimal distance between two adjacent query terms in one document.\n",
    "\tArgs:\n",
    "\t\tdocument: a document matching our query\n",
    "\t\tquery: our initial query given by the user\n",
    "\t\tindex: index containing all terms and matching documents\n",
    "\n",
    "\tReturns: \n",
    "\t\tavg_distance: average minimal distance between adjacent query terms \n",
    "\t\tmax_min_distance: the biggest minimal distance\n",
    "\t\"\"\"\n",
    "\tsum_of_distances = 0\n",
    "\tavg_distance = 0\n",
    "\tmin_distance = math.inf \t\t\t\t\t\t\t# minimal distance between two terms of a query in the document.\n",
    "\tmax_min_distance = 0 \t\t\t\t\t\t\t\t# the maximum minimal distance\n",
    "\tquery_length = len(query)\n",
    "\tnum_comparisons = query_length - 1 \t\t\t\t\t# number of comparisons, needed for normalization\n",
    "\tcount = 0\n",
    "\tfor i in range(query_length -1): \t\t\t\t\t# term in query\n",
    "\t\tmin_distance = math.inf \t\t\t\t\t\t#minimal distance between two terms of a query in the document.\n",
    "\t\tcount +=1\n",
    "\t\tterm_list_1 = []\n",
    "\t\tterm_list_2 = []\n",
    "\n",
    "\t\tfor doc in index[query[i]]:\n",
    "\t\t\tif doc[0] == document:\n",
    "\t\t\t\tterm_list_1 = doc[1] \t\t\t\t\t# get positional lists for document and term one\n",
    "\t\tfor doc in index[query[i+1]]:\n",
    "\t\t\tif doc[0] == document:\n",
    "\t\t\t\tterm_list_2 = doc[1] \t\t\t\t\t# get positional lists for document and term two\n",
    "\t\t# find smallest distance between terms:\n",
    "\t\tfor pos1 in term_list_1:\n",
    "\t\t\tfor pos2 in term_list_2:\n",
    "\t\t\t\tdistance = abs(pos1 - pos2)\n",
    "\t\t\t\tif distance < min_distance:\n",
    "\t\t\t\t\tmin_distance = distance\n",
    "\t\tsum_of_distances += min_distance\n",
    "\t\tif min_distance > max_min_distance: \t\t\t# get maximum minimal distance, used for normalitazion\n",
    "\t\t\tmax_min_distance = min_distance\n",
    "\n",
    "\t# get avg_distance between two terms\n",
    "\tavg_distance = sum_of_distances/num_comparisons \t\t\n",
    "\treturn avg_distance, max_min_distance\n",
    "\n",
    "# work in progress\n",
    "def position_score(documents, query, index):\n",
    "\t\"\"\"\n",
    "\tFunction calculates posiion score for a document-query pair. If terms in documents are near each other, a higher score is given.\n",
    "\tArgs:\n",
    "\t\tdocument: a document matching our query\n",
    "\t\tquery: our initial query given by the user\n",
    "\t\tindex: index containing all terms and matching documents\n",
    "\n",
    "\tReturns: \n",
    "\t\tposition_score_list: list with score for each document, based on how close terms form query are in the document. Score is value between 0 and 1\n",
    "\n",
    "\t\"\"\"\n",
    "\tdistance_score_list = []\n",
    "\tposition_score_list = []\n",
    "\tmax_dist = 1 # distance between two terms can't be smaller than 1\n",
    "\tfor document in documents:\n",
    "\t\tavg_distance, max_min_distance = get_distance(document, query, index)\n",
    "\t\tif max_min_distance > max_dist:\n",
    "\t\t\tmax_dist = max_min_distance\n",
    "\t\tdistance_score_list.append(avg_distance)\n",
    "\n",
    "\tfor distance in distance_score_list:\n",
    "\t\tposition_score_list.append(1 - (distance/max_dist)) # calculate score\n",
    "\n",
    "\treturn position_score_list\n",
    "\n",
    "\n",
    "def ranking_sort_helper(document_and_score):\n",
    "\t\"\"\"\n",
    "\tFunction returning the score of a given document, used for sorting the ranked list.\n",
    "\tArgs:\n",
    "\t\tdocument_and_score: list entry with document and score pair\n",
    "\n",
    "\tReturns: \n",
    "\t\tscore value\n",
    "\t\"\"\"\n",
    "\treturn document_and_score[1]\n",
    "\n",
    "def format_helper_ui(ranking, url_cluster):\n",
    "\t\"\"\"\n",
    "\tFunction formating the ranked list for ui-output. Adding necessary information.\n",
    "\tArgs:\n",
    "\t\tranking: ranked list with documend-id and score pairs \n",
    "\t\turl_cluster: List with document-id, associated URL and cluster.\n",
    "\tReturns: \n",
    "\t\tresult: ranked list with all the necessary information\n",
    "\n",
    "\t\"\"\"\n",
    "\tresult = []\n",
    "\tfor position, entry in enumerate(ranking):\n",
    "\t\tURL = url_cluster[entry[0]][0]\n",
    "\t\tcluster = url_cluster[entry[0]][1]\n",
    "\t\tresult.append([position + 1, URL, entry[0], cluster])\n",
    "\treturn result\n",
    "\n",
    "def format_helper_file(ranking, url_cluster):\n",
    "\t\"\"\"\n",
    "\tFunction formating the ranked list for text-file-output. Adding necessary information.\n",
    "\tArgs:\n",
    "\t\tranking: ranked list with documend-id and score pairs \n",
    "\t\turl_cluster: List with document-id, associated URL and cluster.\n",
    "\tReturns: \n",
    "\t\tresult: ranked list with all the necessary information\n",
    "\n",
    "\t\"\"\"\n",
    "\tresult = []\n",
    "\tfor entry in ranking:\n",
    "\t\tURL = url_cluster[entry[0]][0]\n",
    "\t\tresult.append([URL, entry[1]])\n",
    "\treturn result \n",
    "\n",
    "#Measure the average relevance of a (partial) result list. \n",
    "def measure_avg_relevance(ranking):\n",
    "\t\"\"\"\n",
    "\tFunction calculating the average relevance of a result list\n",
    "\tArgs:\n",
    "\t\tranking: ranked list with documend-id and score pairs \n",
    "\tReturns: \n",
    "\t\taverage relevance score for this list \n",
    "\n",
    "\t\"\"\"\n",
    "\trelevance = 0.0\n",
    "\n",
    "\tfor doc in ranking:\n",
    "\t\trelevance += doc[1]\n",
    "\treturn relevance/len(ranking)\n",
    "\n",
    "#Measure the diversity of a (partial) result list. Diversity is meassured by the difference of sim hash values of those documents\n",
    "def measure_diversity(ranking, hashes):\n",
    "\t\"\"\"\n",
    "\tFunction calculating the diversity of a result list\n",
    "\tArgs:\n",
    "\t\tranking: ranked list with documend-id and score pairs \n",
    "\t\thashes: list of sim hashes, used for calculating similarity\n",
    "\tReturns: \n",
    "\t\tdiversity: diversity score of list\n",
    "\n",
    "\t\"\"\"\n",
    "\tdiversity = 0.0\n",
    "\tsimilarity = 0\n",
    "\thash_length = len(hashes[0])\n",
    "\tfor i in range(hash_length):\n",
    "\t\tis_equal = True\n",
    "\t\tfor j in range(len(ranking) - 1): \t\t\t\t\t\t\t\t# loop over each bit in the sim hash\n",
    "\t\t\tif hashes[ranking[j][0]][i] != hashes[ranking[j+1][0]][i]:  # check if hash of two following documents have the same value at bit j of hash.  \n",
    "\t\t\t\tis_equal = False\t\t\t\t\t\t\t\t\t\t# if two documents do not have the same bit they aren't similar.\n",
    "\t\t\t\tbreak\t\t\t\t\t\t\t\t\t\t\t\t\t# no need for further comparison\n",
    "\t\tif is_equal:\n",
    "\t\t\tsimilarity += 1\t\n",
    "\tdiversity = (hash_length - similarity) / (hash_length)\t\t\t\t# normalize to values between 0 and 1\n",
    "\treturn diversity\n",
    "\n",
    "\n",
    "def diversify(ranking, hashes, num_entries, beta):\n",
    "\t\"\"\"\n",
    "\tFunction diversifies a ranked list\n",
    "\tArgs:\n",
    "\t\tranking: ranked list with documend-id and score pairs \n",
    "\t\thashes: list of sim hashes, used for calculating similarity\n",
    "\t\tnum_entries: number of entries which should be in the diversified list\n",
    "\t\tbeta: parameter which determines ratio between relevance and diversity in   \n",
    "\t\t \n",
    "\tReturns: \n",
    "\t\treranked: reranked result list with document score pairs\n",
    "\n",
    "\t\"\"\"\n",
    "\treranked = []\n",
    "\tif len(ranking) < num_entries:\t# prevent index out of bounce\n",
    "\t\tnum_entries = len(ranking)\n",
    "\n",
    "\treranked.append(ranking[0])\n",
    "\tdel ranking[0]\n",
    "\n",
    "\twhile len(reranked) < num_entries:\t\t\n",
    "\t\tgreedy = 0\n",
    "\t\tindex = 0\n",
    "\t\tfor doc in ranking:\n",
    "\t\t\tpre_list = []\n",
    "\t\t\tpre_list += reranked\n",
    "\t\t\tpre_list.append(doc)\n",
    "\t\t\t\n",
    "\t\t\trelevance = measure_avg_relevance(pre_list)\n",
    "\t\t\tdiversity = measure_diversity(pre_list, hashes)\n",
    "\t\t\tweighted_mix = beta * relevance + (1-beta) * diversity \n",
    "\t\t\tif(weighted_mix > greedy):\n",
    "\t\t\t\tgreedy = weighted_mix\n",
    "\t\t\t\tindex = ranking.index(doc)\n",
    "\t\t\t\n",
    "\t\treranked.append(ranking[index])\n",
    "\t\tdel ranking[index]\t\t\n",
    "\n",
    "\treturn reranked\n",
    "\n",
    "#Retrieve documents relevant to a query. You need (at least) two parameters:\n",
    "    #query: The user's search query\n",
    "    #index: The location of the local index storing the discovered documents.\n",
    "def retrieve(query, index, is_batch=False ):\n",
    "\t\"\"\"\n",
    "\tFunction takes a query and index. Ranks documents matching the query for relevance.  \n",
    "\tArgs:\n",
    "\t\tquery: our initial query given by the user\n",
    "\t\tindex: index containing all terms and matching documents\n",
    "\t\tis_batch: boolean, if true, queries are from text-file, return format differs from UI-query, default is false\n",
    "\n",
    "\tReturns: \n",
    "\t\tresult_list: list of ranked documents\n",
    "\n",
    "\t\"\"\"\n",
    "\tranking = []\n",
    "\tresult_list = []\n",
    "\talpha = 0.7 # alpha value used for weighting bm_25 score, should be between 0 and 1\n",
    "\tbeta = 0.7 # beta value used for weighting relevance score and diversity. Beta should be between 0 and 1\n",
    "\tk = 1.2\n",
    "\tb = 0.75\n",
    "\treranked = []\n",
    "\n",
    "\t# get processed query\n",
    "\tquery_tokenized= prepare_query(query)\t\t\n",
    "\n",
    "\t#get dictionary from index\n",
    "\tdocument_index = index[0]\n",
    "\t#get avg document length:\n",
    "\tavg_length = index[3]\n",
    "\t# get number of documents in index:\n",
    "\tnum_doc = len(index[1])\n",
    "\thashes = index[2]\n",
    "\n",
    "\t# get all the documents, that include all the terms from our query\n",
    "\tdocuments = get_documents_from_index(query_tokenized, document_index)\t\n",
    "\t\n",
    "\t# get position scores\n",
    "\t# get them only if there are more than one term in the query, otherwise can't be calculated.\n",
    "\tif len(query_tokenized) > 1:\n",
    "\t\tpos_scores = position_score(documents, query_tokenized, document_index)\n",
    "\telse:\n",
    "\t\tpos_scores = [1 for _ in range(len(documents))]\n",
    "\n",
    "\tmax_bm25_score = 0\n",
    "\tbm25_score_list = []\n",
    "\t# calculate bm 25 score for each document:\n",
    "\tfor document in documents:\n",
    "\t\tdocument_length = index[1][document][2]\n",
    "\n",
    "\t\t# get Bm25 score\n",
    "\t\tbm25_score = bm25(document, query_tokenized, document_index, k, b, avg_length, document_length, num_doc)\n",
    "\t\tbm25_score_list.append([document, bm25_score])\n",
    "\t\tif bm25_score > max_bm25_score:\n",
    "\t\t\tmax_bm25_score = bm25_score\n",
    "\n",
    "\t# calculate relevance score for each document \n",
    "\tfor i, entry in enumerate(bm25_score_list):\n",
    "\t\t# get position score\n",
    "\t\tpos_score = pos_scores[i]\n",
    "\t\tbm25_score_normalized = entry[1]/max_bm25_score\n",
    "\t\t#calculate score based on bm25 score and position score\n",
    "\t\tscore = (alpha * bm25_score_normalized + (1-alpha) * pos_score)\n",
    "\n",
    "\t\tranking.append([entry[0], score])\n",
    "\n",
    "\t\n",
    "\t# Sort ranking based on relevance score \n",
    "\tranking.sort(key=ranking_sort_helper, reverse=True)\n",
    "\n",
    "\n",
    "\t# diversify result list\n",
    "\tif len(ranking) >=2: #only rerank, if enough items in list \n",
    "\t\treranked = diversify(copy.copy(ranking), hashes, 99, beta)\n",
    "\telse: \n",
    "\t\treranked = ranking\n",
    "\n",
    "\t#format output\n",
    "\tif is_batch:\n",
    "\t\t#get ranking into right format for txt-file:\n",
    "\t\tresult_list = format_helper_file(reranked[:99], index[1])\n",
    "\telse:\n",
    "\t\t#get ranking into right format for srp visualization:\n",
    "\t\tresult_list = format_helper_ui(reranked[:99], index[1])\n",
    "\n",
    "\treturn result_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search Result Presentation\n",
    "Once you have a result set, we want to return it to the searcher in two ways: a) in an interactive user interface. For this user interface, please think of **at least one innovation** that goes beyond the traditional 10-blue-links interface that most commercial search engines employ. b) as a text file used for batch performance evaluation. The text file should be formatted to produce one ranked result per line, listing the query number, rank position, document URL and relevance score as tab-separated entries. An example of the first three lines of such a text file looks like this:\n",
    "\n",
    "```\n",
    "1   1   https://www.tuebingen.de/en/3521.html   0.725\n",
    "1   2   https://www.komoot.com/guide/355570/castles-in-tuebingen-district   0.671\n",
    "1   3   https://www.unimuseum.uni-tuebingen.de/en/museum-at-hohentuebingen-castle   0.529\n",
    "...\n",
    "1   100 https://www.tuebingen.de/en/3536.html   0.178\n",
    "2   1   https://www.tuebingen.de/en/3773.html   0.956\n",
    "2   2   https://www.tuebingen.de/en/4456.html   0.797\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce a text file with 100 results per query in the format specified above.\n",
    "def load_queries(query_file_path):\n",
    "    queries = []\n",
    "    with open(query_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                query_num = int(parts[0])\n",
    "                query_desc = parts[1]\n",
    "                queries.append((query_num, query_desc))\n",
    "    return queries\n",
    "\n",
    "def batch(query_file_path, output_file_path):\n",
    "    queries = load_queries(query_file_path)\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for query_num, query_desc in queries:\n",
    "            results = retrieve(query_desc, index, True)\n",
    "            for rank, result in enumerate(results, start=1):\n",
    "                result_line = f\"{query_num}\\t{rank}\\t{result[0]}\\t{result[1]}\"\n",
    "                output_file.write(result_line + '\\n')\n",
    "\n",
    "query_file_path = 'queries.txt'\n",
    "output_file_path = 'batch_results_Sysala_Hirsch_Wenninger_Moser.txt'\n",
    "batch(query_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:22] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:22] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/6.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/112.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/163.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/754.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/266.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:37] \"\u001b[36mGET /static/pictures/501.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:38] \"\u001b[36mGET /static/pictures/175.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:38] \"\u001b[36mGET /static/pictures/416.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:38] \"\u001b[36mGET /static/pictures/506.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:38] \"\u001b[36mGET /static/pictures/56.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:53] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:53] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/524.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/254.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/170.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/658.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/767.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/88.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:44:59] \"\u001b[36mGET /static/pictures/13.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:00] \"\u001b[36mGET /static/pictures/115.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:00] \"\u001b[36mGET /static/pictures/209.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:00] \"\u001b[36mGET /static/pictures/614.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/633.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/391.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/505.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/484.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/629.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/661.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:23] \"\u001b[36mGET /static/pictures/567.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:24] \"\u001b[36mGET /static/pictures/165.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:24] \"\u001b[36mGET /static/pictures/193.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:24] \"\u001b[36mGET /static/pictures/105.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/520.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/790.svg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/681.png HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/570.svg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/364.svg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/361.svg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"GET /static/pictures/638.svg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"\u001b[36mGET /static/pictures/490.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"\u001b[36mGET /static/pictures/778.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:34] \"\u001b[36mGET /static/pictures/633.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/570.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/520.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/364.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/361.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/633.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/790.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/681.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/638.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/490.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:39] \"\u001b[36mGET /static/pictures/778.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/151.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/428.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/319.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/302.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/648.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:44] \"\u001b[36mGET /static/pictures/626.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:45] \"\u001b[36mGET /static/pictures/215.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:45] \"\u001b[36mGET /static/pictures/680.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:45] \"\u001b[36mGET /static/pictures/39.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:45] \"\u001b[36mGET /static/pictures/193.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/505.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/633.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/629.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/391.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/484.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/165.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/661.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/567.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:49] \"\u001b[36mGET /static/pictures/193.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:45:50] \"\u001b[36mGET /static/pictures/105.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/498.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/421.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/480.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[33mGET /static/pictures/default_picture.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/766.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/324.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:09] \"\u001b[36mGET /static/pictures/622.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[33mGET /static/pictures/default_picture.jpg HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/498.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/421.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/480.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/766.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/324.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:46:26] \"\u001b[36mGET /static/pictures/622.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/6.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/112.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/754.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/56.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/163.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/266.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/501.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/416.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/175.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:10] \"\u001b[36mGET /static/pictures/506.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:12] \"\u001b[32mPOST /filter HTTP/1.1\u001b[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:13] \"GET /search?query=University+of+Tübingen&filter=Religion HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:13] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:13] \"\u001b[36mGET /static/pictures/467.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:16] \"\u001b[32mPOST /filter HTTP/1.1\u001b[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"GET /search?query=University+of+Tübingen&filter=University HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/6.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/266.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/163.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/754.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/501.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/112.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/416.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/56.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/175.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:17] \"\u001b[36mGET /static/pictures/506.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:22] \"\u001b[32mPOST /filter HTTP/1.1\u001b[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:23] \"GET /search?query=University+of+Tübingen&filter=Culture HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:23] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:23] \"\u001b[36mGET /static/pictures/138.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:25] \"\u001b[32mPOST /filter HTTP/1.1\u001b[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:27] \"GET /search?query=University+of+Tübingen&filter=Safety HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:27] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:27] \"\u001b[36mGET /static/pictures/682.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/6.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/266.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/163.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/754.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/501.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/112.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/416.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/56.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/175.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:32] \"\u001b[36mGET /static/pictures/506.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:39] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:48:39] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/385.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/185.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/610.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/697.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/3.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/468.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/538.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/193.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/62.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:49:57] \"\u001b[36mGET /static/pictures/769.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/591.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/766.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/577.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/603.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/676.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/404.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/258.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/398.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:04] \"\u001b[36mGET /static/pictures/497.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/661.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/614.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/657.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/567.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/39.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"GET /static/pictures/188.png HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/767.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/87.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/444.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:08] \"\u001b[36mGET /static/pictures/766.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"POST /search HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/255.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/57.png HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/560.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/262.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/203.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/589.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/353.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/78.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/455.svg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [22/Jul/2024 20:50:16] \"\u001b[36mGET /static/pictures/94.svg HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    }
   ],
   "source": [
    "#Implement an interactive user interface for part a of this exercise.\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Logging configure\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading JSON from {file_path}: {e}\")\n",
    "    return {}\n",
    "\n",
    "# Get the Image of each document\n",
    "def find_image(doc_number):\n",
    "    supported_formats = ['.svg', '.png', '.jpg', '.jpeg']\n",
    "    for fmt in supported_formats:\n",
    "        img_path = f'static/pictures/{doc_number}{fmt}'\n",
    "        if os.path.exists(img_path):\n",
    "            return img_path\n",
    "    return 'static/pictures/default_picture.jpg'\n",
    "\n",
    "# Path categories\n",
    "categories = load_json_file('static/categories.json')\n",
    "document_index = load_json_file('index.json')\n",
    "\n",
    "# Get title and description\n",
    "def get_info(doc_number):\n",
    "    json_path = f'static/documents/{doc_number}.json'\n",
    "    data = load_json_file(json_path)\n",
    "    title = data.get('title', 'No title')\n",
    "    description = data.get('text', 'No description')\n",
    "    image_url = find_image(doc_number)\n",
    "    return title, description, image_url\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html', categories=categories)\n",
    "\n",
    "@app.route('/search', methods=['GET', 'POST'])\n",
    "def search():\n",
    "    query = request.args.get('query') if request.method == 'GET' else request.form['query']\n",
    "    ranklist = retrieve(query, document_index)\n",
    "   \n",
    "    selected_filter = request.args.get('filter') if request.method == 'GET' else request.form.get('filter')\n",
    "    # Apply the filter if selected\n",
    "    if selected_filter:\n",
    "        filtered_ranklist = [result for result in ranklist if result[3] == selected_filter]\n",
    "    else:\n",
    "        filtered_ranklist = ranklist\n",
    "\n",
    "    # Filter counts\n",
    "    filter_counter = Counter([result[3] for result in ranklist])\n",
    "    top_filters = filter_counter.most_common(5)\n",
    "    top_filter_categories = [category for category, _ in top_filters]\n",
    "\n",
    "    # Limit results to 10 for display\n",
    "    display_ranklist = filtered_ranklist[:10]\n",
    "\n",
    "    results = []\n",
    "    for result in display_ranklist:\n",
    "            if selected_filter and result[3] != selected_filter:\n",
    "                continue\n",
    "            doc_number = result[2]\n",
    "            title, description, image_url = get_info(doc_number)\n",
    "            results.append({'url': result[1], 'title': title, 'snippet': description, 'image': image_url})\n",
    "\n",
    "    return render_template('results.html', query=query, results=results, top_filters=top_filter_categories)\n",
    "\n",
    "\n",
    "@app.route('/filter', methods=['POST'])\n",
    "def filter():\n",
    "    selected_filter = request.form['filter']\n",
    "    query = request.form['query']\n",
    "    return redirect(url_for('search', query=query, filter=selected_filter))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance Evaluation \n",
    "We will evaluate the performance of our search systems on the basis of five queries. Two of them are avilable to you now for engineering purposes:\n",
    "- `tübingen attractions`\n",
    "- `food and drinks`\n",
    "\n",
    "The remaining three queries will be given to you during our final session on July 23rd. Please be prepared to run your systems and produce a single result file for all five queries live in class. That means you should aim for processing times of no more than ~1 minute per query. We will ask you to send carsten.eickhoff@uni-tuebingen.de that file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "Your final projects will be graded along the following criteria:\n",
    "- 25% Code correctness and quality (to be delivered on this sheet)\n",
    "- 25% Report (4 pages, PDF, explanation and justification of your design choices)\n",
    "- 25% System performance (based on how well your system performs on the 5 queries relative to the other teams in terms of nDCG)\n",
    "- 15% Creativity and innovativeness of your approach (in particular with respect to your search system #2 and user interface #3 innovations)\n",
    "- 10% Presentation quality and clarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissible libraries\n",
    "You can use any general-puprose ML and NLP libraries such as scipy, numpy, scikit-learn, spacy, nltk, but please stay away from dedicated web crawling or search engine toolkits such as scrapy, whoosh, lucene, terrier, galago and the likes. Pretrained models are fine to use as part of your system, as long as they have not been built/trained for retrieval. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
