{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Group Project\n",
    "\n",
    "Issued: June 11, 2024\n",
    "\n",
    "Due: July 22, 2024\n",
    "\n",
    "Please submit a link to your code base (ideally with a branch that does not change anymore after the submission deadline) and your 4-page report via email to carsten.eickhoff@uni-tuebingen.de by the due date. One submission per team.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Crawling & Indexing\n",
    "Crawl the web to discover **English content related to Tübingen**. The crawled content should be stored locally. If interrupted, your crawler should be able to re-start and pick up the crawling process at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Imports ####\n",
    "import requests\n",
    "from boilerpy3 import extractors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from trafilatura import fetch_url, extract\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from duplicateCheck import check_simhash,computeHash\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser as rp\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import html5lib\n",
    "import requests\n",
    "import stopit\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preprocesses a term t \n",
    "#returns preprocessed t or False in case term is uninformative\n",
    "def preprocess(t, tag):\n",
    " #get nltk stopwords english and german! (often also german stopwords still contained)\n",
    " stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n",
    "\n",
    " #initialize lemmatizer\n",
    " lemmatizer = WordNetLemmatizer()\n",
    "\n",
    " #convert to lowercase\n",
    " t = t.lower()\n",
    "\n",
    " #is url (starting with http(s) or www.)\n",
    " if re.compile(r'https?://\\S+|www\\.\\S+').match(t):\n",
    "    return False\n",
    "      \n",
    " # special character,punctuation -> continue in this case \n",
    " if re.compile(r'[^a-zA-Z0-9äöüÄÖÜ\\s]').match(t):\n",
    "    return False\n",
    " \n",
    " #is stopword\n",
    " if t in stop_words:\n",
    "    return False \n",
    " \n",
    " #remove also special chars inside token\n",
    " t = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s]', '', t)\n",
    "      \n",
    " #convert pos tag for lemmatization\n",
    " ltag = tag[0].lower()\n",
    " if ltag in ['a', 'r', 'n', 'v']:\n",
    "          t = lemmatizer.lemmatize(t, ltag)\n",
    "\n",
    " return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Indexing ####\n",
    "\n",
    "#Add a document to the index. You need (at least) two parameters:\n",
    "    #doc: The document to be indexed.\n",
    "    #index: The location of the local index storing the discovered documents.\n",
    "def index(doc, index):\n",
    " url = doc['url']\n",
    " indexPath = index\n",
    "\n",
    " #get index of index.json\n",
    " with open(indexPath, 'r', encoding='utf-8') as f:\n",
    "     index = json.load(f)\n",
    "     docID = len(index[1])\n",
    "     print(docID)\n",
    "     doc = doc['doc']\n",
    "     f.close()\n",
    "\n",
    "\n",
    "####  Text Preprocessing of the doc ###################\n",
    "\n",
    " #extract relevant text from raw html doc\n",
    " content = extract(doc)\n",
    "\n",
    "\n",
    "###storing unprocessed text content ##### -> store as file at end if no duplicate\n",
    " docContents={}\n",
    " soup = bs(doc, 'html.parser')\n",
    " title = soup.find('title').text if soup.find('title') else 'No title'\n",
    " docContents['title'] = title\n",
    " meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "\n",
    " ###if description available save description, else save first 40 words (or less if doc shorter) -> remove special characters from that ####\n",
    " if meta_desc:\n",
    "     docContents['text'] = meta_desc['content'] \n",
    " else:\n",
    "    splittedText = content.split()\n",
    "    if len(splittedText) >= 40:\n",
    "     docContents['text'] = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s.,?!-]', '', ' '.join(content.split()[:40]))\n",
    "    else: \n",
    "     docContents['text'] = re.sub(r'[^a-zA-Z0-9äöüÄÖÜ\\s.,?!-]', '', ' '.join(splittedText[:len(splittedText)]))\n",
    "\n",
    " #docContents['content'] = content\n",
    " # Find the first image\n",
    " first_image = soup.find('img')\n",
    " \n",
    " \n",
    " #convert to lowercase\n",
    " content = content.lower()\n",
    "\n",
    " #remove contractions:\n",
    " content = contractions.fix(content)\n",
    " \n",
    "\n",
    " #store words of the processed document\n",
    " processedDoc = []\n",
    " #tokenize, go through all words and do the steps for each at once\n",
    " for position, (t, tag) in enumerate(pos_tag(word_tokenize(content))):\n",
    "    \n",
    "      #break up hyphenated words:\n",
    "       if '-' in t:\n",
    "          t = t.split('-')\n",
    "          firstTermPreprocessed = preprocess(t[0], tag)\n",
    "          #if not uninformative\n",
    "          if firstTermPreprocessed:\n",
    "              processedDoc.append([firstTermPreprocessed, position])\n",
    "          secondTermPreprocessed = preprocess(t[1], tag)\n",
    "          if secondTermPreprocessed:\n",
    "              processedDoc.append([secondTermPreprocessed, position])\n",
    "       else:\n",
    "           termPreprocessed = preprocess(t, tag)\n",
    "           if termPreprocessed:\n",
    "               processedDoc.append([termPreprocessed, position])\n",
    "\n",
    "  ############# Near Duplicate checking ##############\n",
    "  #near duplicate checking by means of the words in the processed document (processedDoc)\n",
    "\n",
    " docHash = computeHash([l[0] for l in processedDoc])\n",
    " # compare to other doc hashes\n",
    " if(check_simhash(docHash, index[2])):\n",
    "     #break and dont index if is near duplicate\n",
    "     return [False, 0]\n",
    " #if no duplicate save hash and insert terms in inverted index\n",
    " index[2].append(docHash)\n",
    "\n",
    "\n",
    "\n",
    "############ Build up inverted index #####################\n",
    " for term, position in processedDoc:\n",
    "     \n",
    "      #entry in inverted index is: [docID, [occurence1, occurence2, ...]]\n",
    "      #add on thy fly to inverted index\n",
    "      #if word already in index add doc idto the words list\n",
    "      if(term in index[0].keys()):\n",
    "           #check if doc id already in list of that word (must be at the end)\n",
    "           if index[0][term][len(index[0][term])-1][0] == docID:\n",
    "                #if so append position there\n",
    "                index[0][term][len(index[0][term])-1][1].append(position)\n",
    "           #else add new list [docID, [position of t],skip pointer, tfidfval] for that word\n",
    "           else:\n",
    "               index[0][term].append([docID, [position], None, None])\n",
    "               \n",
    "      #if word not yet in index add \"t: [[docID, [get position of t], tfidf weight for t in d, skip pointer (None) , tfidfval]]\" to dict\n",
    "      else:\n",
    "           index[0][term] = [[docID, [position], None, None]]\n",
    "\n",
    " length = len(processedDoc)\n",
    "\n",
    " #add url , cluster of document (None so far) and length of preprocessed doc to list index[1] after indexing the doc\n",
    " index[1].append([url, None, length])\n",
    "\n",
    " #write changed index object to json\n",
    " with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "     json.dump(index, f, ensure_ascii=False)\n",
    " \n",
    " #save doc in documents ordner: name <docID>.json\n",
    " with open(os.path.join(os.getcwd(),\"documents\", str(docID) + \".json\"), 'w', encoding='utf-8') as f:\n",
    "     json.dump(docContents, f, ensure_ascii=False)\n",
    " \n",
    " #save first image of page if found\n",
    " if first_image:\n",
    "     img_url = first_image['src']\n",
    "     # Convert relative URL to absolute URL\n",
    "     absolute_image_url = urljoin(url, img_url)\n",
    "     f_ext = os.path.splitext(absolute_image_url)[-1]\n",
    "     image_fetched = requests.get(absolute_image_url)\n",
    "     if image_fetched.status_code == 200:\n",
    "       # Get the content of the image\n",
    "        image_content = image_fetched.content\n",
    "       \n",
    "        #save img in pictures ordner: name <docID>\n",
    "        with open(os.path.join(os.getcwd(),\"pictures\", str(docID)) + f_ext, 'wb') as f:\n",
    "           f.write(image_content)\n",
    "\n",
    " return [True, docID]\n",
    " \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Clustering ####\n",
    "#clusters the docs of the index and inserts the labels into the index (currently 30 clusters)\n",
    "# cluster using kmeans clustering with tf-idf vectors\n",
    "def cluster(index):\n",
    "     print('start clustering')\n",
    "     indexPath = index\n",
    "     #get index of index.json\n",
    "     f = open(indexPath, encoding='utf-8')\n",
    "     index = json.load(f)\n",
    "     #convert index to tf-idf vector representations for each document\n",
    "     idx = index[0] \n",
    "     docs = index[1]\n",
    "     #matrix to store vectors (rows: documents , cols: terms)\n",
    "     tfIDFMatrix = np.zeros((len(docs), len(idx.keys())))\n",
    "     print('buildMatrix')\n",
    "\n",
    "     for t in range(len(idx.keys())):\n",
    "         term = list(idx.keys())[t]\n",
    "         for i in range(len(idx[list(idx.keys())[t]])):\n",
    "             #term index: t, doc index: idx[t][i][0]\n",
    "\n",
    "             docID = idx[term][i][0]\n",
    "             \n",
    "             #calculate tf (nr occurences of t in doc/ length of doc) * idf(log (#docs/ #docs containing t))\n",
    "             # occurences of t in doc: len(idx[term][i][1])\n",
    "             # length of doc: docs[docID][2]\n",
    "             # # docs = len[docs]\n",
    "             # #docs containing term = len(idx[term])\n",
    "             tfValue = len(idx[term][i][1])/docs[docID][2]\n",
    "             idfValue = math.log(len(docs)/len(idx[term]))\n",
    "             tfIDFMatrix[docID][t] = tfValue * idfValue\n",
    "\n",
    "             ## store idf value for that doc and term in index on the fly (for BM25 later on)\n",
    "             index[0][term][i][3] = idfValue\n",
    "     \n",
    "     print('endBuildmatrix')\n",
    "\n",
    "     #also calculate avg doc length and append to index\n",
    "     docLengths = [l[2] for l in index[1]]\n",
    "     avgLength = sum(docLengths)/len(docLengths)\n",
    "     index[3] = avgLength\n",
    "      \n",
    "     #place skip pointers once after whole index is built up\n",
    "     #### rearange skip pointers ###########\n",
    "     #delete current skip pointers for that posting list\n",
    "     for term in index[0].keys():\n",
    "        # sqare |p| evenly spaced per posting list (p: length of posting list of term t)\n",
    "        p = len(index[0][term])\n",
    "        #just if posting list has at least length 4\n",
    "        if(p >= 4):\n",
    "            nrPointers = math.sqrt(p)\n",
    "            spaceBetween = math.floor(p/nrPointers)\n",
    "            #current index in postingslist\n",
    "            i = 0\n",
    "            while(i + spaceBetween < p):\n",
    "                #set skip pointer [idx to jump to in postings list, docID at that index]\n",
    "                index[0][term][i][2] = [i + spaceBetween, index[0][term][i + spaceBetween][0]]\n",
    "                i += spaceBetween\n",
    "\n",
    "     #store new  index with tf idf values \n",
    "     #write changed index object to json\n",
    "     with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "      json.dump(index, f, ensure_ascii=False)\n",
    "     # Initialize KMeans clustering\n",
    "     print('clustering')\n",
    "     num_clusters = 10  # Example: Number of clusters\n",
    "     kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "     # Fit KMeans model to the TF-IDF matrix\n",
    "     kmeans.fit(tfIDFMatrix)\n",
    "\n",
    "     docLabels = kmeans.labels_\n",
    "     #centroids of each cluster:\n",
    "     centroids = kmeans.cluster_centers_\n",
    "     #get term with highest tf-idf score per cluster centroid\n",
    "     centroidTopics = []\n",
    "     for centroid in centroids:\n",
    "        centroidTopics.append(list(idx.keys())[np.argmax(centroid)])\n",
    "     \n",
    "     #insert labels in index\n",
    "     for d in range(len(index[1])):\n",
    "         index[1][d][1] = centroidTopics[docLabels[d]]\n",
    "      \n",
    "     print('endClustering')\n",
    "        \n",
    "      #write changed index object to json\n",
    "     with open(indexPath, 'w', encoding='utf-8') as f:\n",
    "      json.dump(index, f, ensure_ascii=False)\n",
    "     \n",
    "     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Crawling ####\n",
    "#Crawl the web. You need (at least) two parameters:\n",
    "#frontier: The frontier of known URLs to crawl. You will initially populate this with your seed set of URLs and later maintain all discovered (but not yet crawled) URLs here.\n",
    "#index: The location of the local index storing the discovered documents. \n",
    "STORAGE_LOC = \"index.json\"\n",
    "MAX_DOCS = 5\n",
    "frontier = [\"https://tuebingenresearchcampus.com/en/tuebingen\",\n",
    "            \"https://tunewsinternational.com/category/news-in-english/\",\n",
    "            \"https://www.tuebingen.de/en/\",\n",
    "            \"https://uni-tuebingen.de/en/\",\n",
    "            \"https://www.germany.travel/en/cities-culture/tuebingen.html\",\n",
    "            \"https://www.iwm-tuebingen.de/www/en/index.html\",\n",
    "             \"https://kunsthalle-tuebingen.de/en/\",\n",
    "             \"https://www.opentable.com/food-near-me/stadt-tubingen-germany\",\n",
    "             \"https://historicgermany.travel/historic-germany/tubingen/\"\n",
    "             \"https://en.wikipedia.org/wiki/T%C3%BCbingen\",\n",
    "              \"https://en.wikipedia.org/wiki/University_of_T%C3%BCbingen\"]\n",
    "links_seen = frontier.copy()\n",
    "\n",
    "def crawl(frontier, indexPath):\n",
    "    \n",
    "    #get  first document of frontier while frontier not empty\n",
    "    while len(frontier) != 0:\n",
    "        with stopit.ThreadingTimeout(20) as context_manager:\n",
    "            link = frontier.pop(0)\n",
    "            try:\n",
    "                base_link = urlparse(link).netloc\n",
    "                \n",
    "                # check if we are allowed to access the website\n",
    "                robots_file_loc = \"http://\" + base_link + \"/robots.txt\"\n",
    "                session = requests.Session()\n",
    "                retry = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('https://', adapter)\n",
    "                robots_file = session.get(robots_file_loc, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"})\n",
    "                session.close()\n",
    "                if robots_file.ok:\n",
    "                    robot_parser = rp()\n",
    "                    robot_parser.set_url(robots_file_loc)\n",
    "                    robot_parser.read()\n",
    "                    if not robot_parser.can_fetch(\"*\", link):\n",
    "                        continue\n",
    "                elif robots_file.status_code != 404:\n",
    "                    continue\n",
    "                \n",
    "                session = requests.Session()\n",
    "                retry = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "                adapter = HTTPAdapter(max_retries=retry)\n",
    "                session.mount('https://', adapter)\n",
    "                document = session.get(link, headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}).text\n",
    "                session.close()\n",
    "                html5_doc = html5lib.parse(document)\n",
    "                soup = bs(document, \"html.parser\")\n",
    "\n",
    "                # save and use document if it's not a duplicate\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract() \n",
    "\n",
    "                # check if document is relevant\n",
    "                doc_lang = html5_doc.get(\"lang\")\n",
    "                if doc_lang == None:\n",
    "                    doc_lang = html5_doc.get(\"xml:lang\")\n",
    "                if doc_lang == None:\n",
    "                    continue\n",
    "                relevant = soup.find(\"body\").text.find(\"Tübingen\") > -1 # type: ignore\n",
    "                if (doc_lang.count(\"en\") > 0 and relevant): # type: ignore\n",
    "                    # process doc to right format and index\n",
    "                    doc = {\"url\": link, \"doc\": document }\n",
    "                    # TODO: index document\n",
    "                    duplicate, docAmount = index(doc, indexPath)\n",
    "                    if duplicate == False:\n",
    "                        print(\"Duplicate\")\n",
    "                        continue\n",
    "                    # get all links from document and save to frontier if not seen yet\n",
    "                    for a in soup.find_all('a'):\n",
    "                        if (a.get('href')):\n",
    "                            l = a.get('href')\n",
    "                            if l.startswith('#'):\t\n",
    "                                continue\n",
    "                            if urlparse(l).netloc == '':\n",
    "                                l = base_link + l\n",
    "                            if urlparse(l).scheme == '':\n",
    "                                l = urlparse(link).scheme + '://' + l\n",
    "                            if l not in links_seen:\n",
    "                                frontier.append(l)\n",
    "                                links_seen.append(l)\n",
    "                    if docAmount >= MAX_DOCS:\n",
    "                        break\n",
    "            except Exception as err:\n",
    "                if context_manager.state == context_manager.TIMED_OUT:\n",
    "                    print(\"Timed out at link: \", link)\n",
    "                else:\n",
    "                    print(\"Exception occured at link: \", link, \". Description: \", err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     json_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m### crawl ####\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     crawl(frontier, STORAGE_LOC)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#after crawling cluster and add remaining infos to index\u001b[39;00m\n\u001b[0;32m     21\u001b[0m cluster(STORAGE_LOC)\n",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m, in \u001b[0;36mcrawl\u001b[1;34m(frontier, indexPath)\u001b[0m\n\u001b[0;32m     37\u001b[0m robot_parser \u001b[38;5;241m=\u001b[39m rp()\n\u001b[0;32m     38\u001b[0m robot_parser\u001b[38;5;241m.\u001b[39mset_url(robots_file_loc)\n\u001b[1;32m---> 39\u001b[0m robot_parser\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m robot_parser\u001b[38;5;241m.\u001b[39mcan_fetch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, link):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\robotparser.py:62\u001b[0m, in \u001b[0;36mRobotFileParser.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reads the robots.txt URL and feeds it to the parser.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     f \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m401\u001b[39m, \u001b[38;5;241m403\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:557\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    555\u001b[0m     http_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    556\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, proto, meth_name) \u001b[38;5;241m+\u001b[39m args\n\u001b[1;32m--> 557\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:749\u001b[0m, in \u001b[0;36mHTTPRedirectHandler.http_error_302\u001b[1;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[0;32m    746\u001b[0m fp\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    747\u001b[0m fp\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mopen(new, timeout\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    537\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1392\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context, check_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_hostname)\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\urllib\\request.py:1317\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno host given\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# will parse host:port\u001b[39;00m\n\u001b[1;32m-> 1317\u001b[0m h \u001b[38;5;241m=\u001b[39m http_class(host, timeout\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttp_conn_args)\n\u001b[0;32m   1318\u001b[0m h\u001b[38;5;241m.\u001b[39mset_debuglevel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debuglevel)\n\u001b[0;32m   1320\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(req\u001b[38;5;241m.\u001b[39munredirected_hdrs)\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\http\\client.py:1425\u001b[0m, in \u001b[0;36mHTTPSConnection.__init__\u001b[1;34m(self, host, port, key_file, cert_file, timeout, source_address, context, check_hostname, blocksize)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcert_file \u001b[38;5;241m=\u001b[39m cert_file\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1425\u001b[0m     context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_default_https_context()\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# send ALPN extension to indicate HTTP/1.1 protocol\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m11\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\ssl.py:775\u001b[0m, in \u001b[0;36mcreate_default_context\u001b[1;34m(purpose, cafile, capath, cadata)\u001b[0m\n\u001b[0;32m    770\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_verify_locations(cafile, capath, cadata)\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mverify_mode \u001b[38;5;241m!=\u001b[39m CERT_NONE:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[1;32m--> 775\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs(purpose)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# OpenSSL 1.1.1 keylog file\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeylog_filename\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wenni\\anaconda3\\Lib\\ssl.py:597\u001b[0m, in \u001b[0;36mSSLContext.load_default_certs\u001b[1;34m(self, purpose)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m storename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_windows_cert_stores:\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_windows_store_certs(storename, purpose)\n\u001b[1;32m--> 597\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_default_verify_paths()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### Execution (crawling, indexing, clustering) ####\n",
    "## delete content of documents and pictures folder if there is some\n",
    "# so the process can be started again without resetting everything by hand\n",
    "docs = [os.path.join(os.getcwd(), \"documents\", f) for f in os.listdir(os.path.join(os.getcwd(), \"documents\"))]\n",
    "for f in docs:\n",
    "    os.remove(f)\n",
    "\n",
    "images = [os.path.join(os.getcwd(), \"pictures\", f) for f in os.listdir(os.path.join(os.getcwd(), \"pictures\"))]\n",
    "for i in images:\n",
    "    os.remove(i)\n",
    "\n",
    "#write [{}, [], [], None] in index.json \n",
    "json_data = json.dumps([{}, [], [], None])\n",
    "with open(STORAGE_LOC, 'w') as json_file:\n",
    "    json_file.write(json_data)\n",
    "    json_file.close()\n",
    "\n",
    "### crawl ####\n",
    "    crawl(frontier, STORAGE_LOC)\n",
    "#after crawling cluster and add remaining infos to index\n",
    "cluster(STORAGE_LOC)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Processing \n",
    "Process a textual query and return the 100 most relevant documents from your index. Please incorporate **at least one retrieval model innovation** that goes beyond BM25 or TF-IDF. Please allow for queries to be entered either individually in an interactive user interface (see also #3 below), or via a batch file containing multiple queries at once. The batch file will be formatted to have one query per line, listing the query number, and query text as tab-separated entries. An example of the batch file for the first two queries looks like this:\n",
    "\n",
    "```\n",
    "1   tübingen attractions\n",
    "2   food and drinks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve documents relevnt to a query. You need (at least) two parameters:\n",
    "    #query: The user's search query\n",
    "    #index: The location of the local index storing the discovered documents.\n",
    "def retrieve(query, index):\n",
    "    #TODO: Implement me\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search Result Presentation\n",
    "Once you have a result set, we want to return it to the searcher in two ways: a) in an interactive user interface. For this user interface, please think of **at least one innovation** that goes beyond the traditional 10-blue-links interface that most commercial search engines employ. b) as a text file used for batch performance evaluation. The text file should be formatted to produce one ranked result per line, listing the query number, rank position, document URL and relevance score as tab-separated entries. An example of the first three lines of such a text file looks like this:\n",
    "\n",
    "```\n",
    "1   1   https://www.tuebingen.de/en/3521.html   0.725\n",
    "1   2   https://www.komoot.com/guide/355570/castles-in-tuebingen-district   0.671\n",
    "1   3   https://www.unimuseum.uni-tuebingen.de/en/museum-at-hohentuebingen-castle   0.529\n",
    "...\n",
    "1   100 https://www.tuebingen.de/en/3536.html   0.178\n",
    "2   1   https://www.tuebingen.de/en/3773.html   0.956\n",
    "2   2   https://www.tuebingen.de/en/4456.html   0.797\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement an interactive user interface for part a of this exercise.\n",
    "\n",
    "#Produce a text file with 100 results per query in the format specified above.\n",
    "def batch(results):\n",
    "    #TODO: Implement me.    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance Evaluation \n",
    "We will evaluate the performance of our search systems on the basis of five queries. Two of them are avilable to you now for engineering purposes:\n",
    "- `tübingen attractions`\n",
    "- `food and drinks`\n",
    "\n",
    "The remaining three queries will be given to you during our final session on July 23rd. Please be prepared to run your systems and produce a single result file for all five queries live in class. That means you should aim for processing times of no more than ~1 minute per query. We will ask you to send carsten.eickhoff@uni-tuebingen.de that file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "Your final projects will be graded along the following criteria:\n",
    "- 25% Code correctness and quality (to be delivered on this sheet)\n",
    "- 25% Report (4 pages, PDF, explanation and justification of your design choices)\n",
    "- 25% System performance (based on how well your system performs on the 5 queries relative to the other teams in terms of nDCG)\n",
    "- 15% Creativity and innovativeness of your approach (in particular with respect to your search system #2 and user interface #3 innovations)\n",
    "- 10% Presentation quality and clarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissible libraries\n",
    "You can use any general-puprose ML and NLP libraries such as scipy, numpy, scikit-learn, spacy, nltk, but please stay away from dedicated web crawling or search engine toolkits such as scrapy, whoosh, lucene, terrier, galago and the likes. Pretrained models are fine to use as part of your system, as long as they have not been built/trained for retrieval. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
