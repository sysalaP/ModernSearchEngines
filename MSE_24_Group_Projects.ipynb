{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Group Project\n",
    "\n",
    "Issued: June 11, 2024\n",
    "\n",
    "Due: July 22, 2024\n",
    "\n",
    "Please submit a link to your code base (ideally with a branch that does not change anymore after the submission deadline) and your 4-page report via email to carsten.eickhoff@uni-tuebingen.de by the due date. One submission per team.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Crawling & Indexing\n",
    "Crawl the web to discover **English content related to Tübingen**. The crawled content should be stored locally. If interrupted, your crawler should be able to re-start and pick up the crawling process at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import justext\n",
    "from boilerpy3 import extractors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "from trafilatura import fetch_url, extract\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Add a document to the index. You need (at least) two parameters:\n",
    "\t#doc: The document to be indexed.\n",
    "\t#index: The location of the local index storing the discovered documents.\n",
    "def index(doc, index):\n",
    "    #TODO: Implement me\n",
    "\tpass\n",
    "\n",
    "#Crawl the web. You need (at least) two parameters:\n",
    "\t#frontier: The frontier of known URLs to crawl. You will initially populate this with your seed set of URLs and later maintain all discovered (but not yet crawled) URLs here.\n",
    "\t#index: The location of the local index storing the discovered documents. \n",
    "def crawl(frontier, index):\n",
    "    #TODO: Implement me\n",
    "\tpass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Query Processing \n",
    "Process a textual query and return the 100 most relevant documents from your index. Please incorporate **at least one retrieval model innovation** that goes beyond BM25 or TF-IDF. Please allow for queries to be entered either individually in an interactive user interface (see also #3 below), or via a batch file containing multiple queries at once. The batch file will be formatted to have one query per line, listing the query number, and query text as tab-separated entries. An example of the batch file for the first two queries looks like this:\n",
    "\n",
    "```\n",
    "1   tübingen attractions\n",
    "2   food and drinks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tübingen', 'attractions']\n",
      "tübingen\n",
      "attraction\n",
      "[[0, None], [1, None], [2, None], [12, None], [13, None], [14, None]]\n",
      "[[0, None], [1, None], [2, None], [14, None]]\n",
      "[[0, None], [14, None]]\n",
      "erhaltene Dokumente [0, 14]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "# Process query\n",
    "# remove stop words, lemmatize, etc.\n",
    "# Todo: Sonderzeichen, bindestrich wörter aufteilen\n",
    "def prepare_query(query):\n",
    "\tquery_tokenized= []\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tquery_lower = query.lower()\n",
    " \t\n",
    "\tfor term, tag in pos_tag(word_tokenize(query_lower)):\n",
    "\t\tif re.compile(r'[^a-zA-Z0-9äöüÄÖÜ\\s]').match(term):\n",
    "\t\t\tcontinue\n",
    "\t\tif term in stop_words:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# split words with \"-\"\n",
    "\t\tif '-' in term:\n",
    "\t\t\tt = term.split('-')\n",
    "\t\t\tfor i, tag in pos_tag(t):\n",
    "\t\t\t\tltag = tag[0].lower()\n",
    "\t\t\t\tif ltag in ['a', 'r', 'n', 'v']:\n",
    "\t\t\t\t\ti = lemmatizer.lemmatize(i, ltag)\t\n",
    "\t\t\t\t\tif i not in query_tokenized:\n",
    "\t\t\t\t\t\tquery_tokenized.append(i)\n",
    "\t\telse: # words without \"-\"\n",
    "\t\t\tltag = tag[0].lower()\n",
    "\t\t\tif ltag in ['a', 'r', 'n', 'v']:\n",
    "\t\t\t\tterm = lemmatizer.lemmatize(term, ltag)\t\n",
    "\t\t\t\tif term not in query_tokenized:\n",
    "\t\t\t\t\tquery_tokenized.append(term)\n",
    "\treturn query_tokenized\n",
    "\n",
    "#get all the documents that include all the queryterms from our index \n",
    "def get_documents_from_index(query_tokenized, index):\n",
    "\tdocuments = []\n",
    "\tfirst_document = True\n",
    "\tsecond_document = False\n",
    "\n",
    "\tfor term in query_tokenized:\n",
    "\t\t# first term in query --> get all documents amtching this term from index\n",
    "\t\tif term in index.keys() and first_document:   \t\t\n",
    "\t\t\tfirst_document = False\t\n",
    "\t\t\tdocuments = index[term]\n",
    "\t\t\tsecond_document = True\n",
    "\t\t# check second term. get documents from first iteration which have this term as well. use skip pointers for both lists \n",
    "\t\telif term in index.keys() and second_document: \n",
    "\t\t\tsecond_document = False\n",
    "\t\t\tmatches = []\n",
    "\t\t\tnew_term_list = index[term]\n",
    "\t\t\ti = 0\n",
    "\t\t\tj = 0\n",
    "\t\t\twhile i < len(documents) and j < len(new_term_list):  \n",
    "\t\t\t\tif documents[i][0] == new_term_list[j][0]:\n",
    "\t\t\t\t\tmatches.append(documents[i])\t\t\t\t\t\t\t# append entry including skip pointers for future iterations \n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tj += 1\n",
    "\t\t\t\telif documents[i][0] < new_term_list[j][0]:                 # A < B\n",
    "\t\t\t\t\tif documents[i][1]:                                     # If there is a skip pointer\n",
    "\t\t\t\t\t\tif documents[i][1][1] <= new_term_list[j][0]:       # Take it if it does not carry you beyond the id pointed to by B\n",
    "\t\t\t\t\t\t\ti = documents[i][1][0]\n",
    "\t\t\t\t\t\telse:                                       \t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\t\t\t\ti += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ti += 1\n",
    "\t\t\t\telif new_term_list[j][1]:                                   # If there is a skip pointer\n",
    "\t\t\t\t\tif new_term_list[j][1][1] <= documents[i][0]:           # Take it if it does not carry you beyond the id pointed to by A\n",
    "\t\t\t\t\t\tj = new_term_list[j][1][0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tj += 1                                     \t\t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tj += 1 \n",
    "\t\t\tdocuments = matches\t\t\n",
    "\t\t\t\n",
    "\t\t# check current term. get documents from previous iteration which have this term as well. use skip pointers only for list in index.\n",
    "\t\telif term in index.keys():\n",
    "\t\t\tmatches = []\n",
    "\t\t\tnew_term_list = index[term]\n",
    "\t\t\ti = 0\n",
    "\t\t\tj = 0\n",
    "\t\t\twhile i < len(documents) and j < len(new_term_list):  \n",
    "\t\t\t\tif documents[i][0] == new_term_list[j][0]:\n",
    "\t\t\t\t\tmatches.append(documents[i])\t\t\t\t\t\t\t# append entry including skip pointers for future iterations \n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\t\t\tj += 1\n",
    "\t\t\t\telif documents[i][0] < new_term_list[j][0]:    \t\t\t\t# A < B\n",
    "\t\t\t\t\ti += 1\t\t\t\t\t\t\t\t\t\t\t\t\t# This time no skip pointers, because they aren't correct anymore\n",
    "\n",
    "\t\t\t\telif new_term_list[j][1]:                                   # If there is a skip pointer\n",
    "\t\t\t\t\tif new_term_list[j][1][1] <= documents[i][0]:           # Take it if it does not carry you beyond the id pointed to by A\n",
    "\t\t\t\t\t\tj = new_term_list[j][1][0]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tj += 1                                     \t\t\t# Otherwise increase the pointer by 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tj += 1 \n",
    "\t\t\tdocuments = matches\t\t\n",
    "\t\telse:\n",
    "\t\t\tdocuments = []\n",
    "\t\t\tbreak\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# query term wasn't found, no document can satisfy query\n",
    "\n",
    "\t\tprint(documents)\t\n",
    "\n",
    "\t# get rid of skip pointers:\n",
    "\ttmp = []\n",
    "\tfor element in documents:\n",
    "\t\ttmp.append(element[0])\n",
    "\tdocuments = tmp\n",
    "\n",
    "\t# old code: remove later\n",
    "\t#for term in query_tokenized:\n",
    "\t#\tdocuments_term = []\n",
    "\t#\tif term in index.keys():\n",
    "\t#\t\tset_first_document = True\n",
    "\t#\t\tfor document_entry in index[term]:\n",
    "\t#\t\t\tdocuments_term.append(document_entry[0])\n",
    "\t#\tif first_document:\n",
    "\t#\t\tdocuments = documents_term\n",
    "\t#\telse:\n",
    "\t#\t\tdocuments = list(set(documents).intersection(documents_term))\n",
    "\t#\tif (set_first_document):\n",
    "\t#\t\tfirst_document = False # Todo: anders lösen\n",
    "\treturn documents\n",
    "\n",
    "#Retrieve documents relevnt to a query. You need (at least) two parameters:\n",
    "\t#query: The user's search query\n",
    "\t#index: The location of the local index storing the discovered documents.\n",
    "def retrieve(query, index):\n",
    "    #TODO: Implement me\n",
    "\n",
    "\t# get processed query\n",
    "\tquery_tokenized= prepare_query(query)\t\t\n",
    "\n",
    "\t#get dictionary from indes\n",
    "\tdocument_index = index[0]\n",
    "\n",
    "\t# get all the documents, that include all the terms from our query\n",
    "\tdocuments = get_documents_from_index(query_tokenized, document_index)\n",
    "\t\n",
    "\tprint('erhaltene Dokumente', documents)\n",
    "\n",
    "\t# Todo: implement retreival model\n",
    "\t# Use generative model, query likelihood model?\n",
    "\t# Todo: implemenrt ranking\n",
    "\n",
    "\n",
    "\t# diversify: use hash\n",
    "\thashes = index[2]\n",
    "\t# Todo: implement diversify function\n",
    "\n",
    "#[['D0', [2, 'D2']], ['D1', None], ['D2', None], ['D14', None]]\n",
    "test_index = ({'tübingen': [[0,[2,2]],[1,None],[2,None], [14,None]],\n",
    "\t\t\t 'henry': [[0,None],[1,None],[2,None],[12,None],[13,None], [14,None]],\n",
    "\t\t\t 'attraction': [[0,[2,12]],[3,[3,14]],[12,None],[14,None]]},\n",
    "\t\t\t [['URL0'],['URL1'],['URL2']],\n",
    "\t\t\t ['Hash1','Hash1'])\n",
    "retrieve('henry, tübingen-attractions' , test_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Search Result Presentation\n",
    "Once you have a result set, we want to return it to the searcher in two ways: a) in an interactive user interface. For this user interface, please think of **at least one innovation** that goes beyond the traditional 10-blue-links interface that most commercial search engines employ. b) as a text file used for batch performance evaluation. The text file should be formatted to produce one ranked result per line, listing the query number, rank position, document URL and relevance score as tab-separated entries. An example of the first three lines of such a text file looks like this:\n",
    "\n",
    "```\n",
    "1   1   https://www.tuebingen.de/en/3521.html   0.725\n",
    "1   2   https://www.komoot.com/guide/355570/castles-in-tuebingen-district   0.671\n",
    "1   3   https://www.unimuseum.uni-tuebingen.de/en/museum-at-hohentuebingen-castle   0.529\n",
    "...\n",
    "1   100 https://www.tuebingen.de/en/3536.html   0.178\n",
    "2   1   https://www.tuebingen.de/en/3773.html   0.956\n",
    "2   2   https://www.tuebingen.de/en/4456.html   0.797\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement an interactive user interface for part a of this exercise.\n",
    "\n",
    "#Produce a text file with 100 results per query in the format specified above.\n",
    "def batch(results):\n",
    "    #TODO: Implement me.    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Performance Evaluation \n",
    "We will evaluate the performance of our search systems on the basis of five queries. Two of them are avilable to you now for engineering purposes:\n",
    "- `tübingen attractions`\n",
    "- `food and drinks`\n",
    "\n",
    "The remaining three queries will be given to you during our final session on July 23rd. Please be prepared to run your systems and produce a single result file for all five queries live in class. That means you should aim for processing times of no more than ~1 minute per query. We will ask you to send carsten.eickhoff@uni-tuebingen.de that file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "Your final projects will be graded along the following criteria:\n",
    "- 25% Code correctness and quality (to be delivered on this sheet)\n",
    "- 25% Report (4 pages, PDF, explanation and justification of your design choices)\n",
    "- 25% System performance (based on how well your system performs on the 5 queries relative to the other teams in terms of nDCG)\n",
    "- 15% Creativity and innovativeness of your approach (in particular with respect to your search system #2 and user interface #3 innovations)\n",
    "- 10% Presentation quality and clarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permissible libraries\n",
    "You can use any general-puprose ML and NLP libraries such as scipy, numpy, scikit-learn, spacy, nltk, but please stay away from dedicated web crawling or search engine toolkits such as scrapy, whoosh, lucene, terrier, galago and the likes. Pretrained models are fine to use as part of your system, as long as they have not been built/trained for retrieval. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
